{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Mission ##\n",
    "\n",
    "Spam detection is one of the major applications of Machine Learning in the interwebs today. Pretty much all of the major email service providers have spam detection systems built in and automatically classify such mail as 'Junk Mail'. \n",
    "\n",
    "In this mission we will be using the Naive Bayes algorithm to create a model that can classify SMS messages as spam or not spam, based on the training we give to the model. It is important to have some level of intuition as to what a spammy text message might look like. Often they have words like 'free', 'win', 'winner', 'cash', 'prize' and the like in them as these texts are designed to catch your eye and in some sense tempt you to open them. Also, spam messages tend to have words written in all capitals and also tend to use a lot of exclamation marks. To the human recipient, it is usually pretty straightforward to identify a spam text and our objective here is to train a model to do that for us!\n",
    "\n",
    "Being able to identify spam messages is a binary classification problem as messages are classified as either 'Spam' or 'Not Spam' and nothing else. Also, this is a supervised learning problem, as we will be feeding a labelled dataset into the model, that it can learn from, to make future predictions. \n",
    "\n",
    "# Overview\n",
    "\n",
    "This project has been broken down in to the following steps: \n",
    "\n",
    "- Step 0: Introduction to the Naive Bayes Theorem\n",
    "- Step 1.1: Understanding our dataset\n",
    "- Step 1.2: Data Preprocessing\n",
    "- Step 2.1: Bag of Words (BoW)\n",
    "- Step 2.2: Implementing BoW from scratch\n",
    "- Step 2.3: Implementing Bag of Words in scikit-learn\n",
    "- Step 3.1: Training and testing sets\n",
    "- Step 3.2: Applying Bag of Words processing to our dataset.\n",
    "- Step 4.1: Bayes Theorem implementation from scratch\n",
    "- Step 4.2: Naive Bayes implementation from scratch\n",
    "- Step 5: Naive Bayes implementation using scikit-learn\n",
    "- Step 6: Evaluating our model\n",
    "- Step 7: Conclusion\n",
    "\n",
    "**Note**: If you need help with a step, you can find the solution notebook by clicking on the Jupyter logo in the top left of the notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Introduction to the Naive Bayes Theorem ###\n",
    "\n",
    "Bayes Theorem is one of the earliest probabilistic inference algorithms. It was developed by Reverend Bayes (which he used to try and infer the existence of God no less), and still performs extremely well for certain use cases. \n",
    "\n",
    "In short, Bayes Theorem calculates the probability of a certain event happening (in our case, a message being spam) based on the joint probabilistic distributions of certain other events (in our case, the appearance of certain words in a message). We will dive into the workings of Bayes Theorem later in the mission, but first, let us understand the data we are going to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Understanding our dataset ### \n",
    "\n",
    "\n",
    "We will be using a dataset originally compiled and posted on the UCI Machine Learning repository which has a very good collection of datasets for experimental research purposes. If you're interested, you can review the [abstract](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) and the original [compressed data file](https://archive.ics.uci.edu/ml/machine-learning-databases/00228/) on the UCI site. For this exercise, however, we've gone ahead and downloaded the data for you.\n",
    "\n",
    "\n",
    " **Here's a preview of the data:** \n",
    "\n",
    "<img src=\"3.jpg\">\n",
    "\n",
    "The columns in the data set are currently not named and as you can see, there are 2 columns. \n",
    "\n",
    "The first column takes two values, 'ham' which signifies that the message is not spam, and 'spam' which signifies that the message is spam. \n",
    "\n",
    "The second column is the text content of the SMS message that is being classified."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    ">**Instructions:**\n",
    "* Import the dataset into a pandas dataframe using the **read_table** method. The file has already been downloaded, and you can access it using the filepath 'smsspamcollection/SMSSpamCollection'. Because this is a tab separated dataset we will be using '\\\\t' as the value for the 'sep' argument which specifies this format. \n",
    "* Also, rename the column names by specifying a list ['label', 'sms_message'] to the 'names' argument of read_table().\n",
    "* Print the first five values of the dataframe with the new column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                        sms_message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Dataset available using filepath 'smsspamcollection/SMSSpamCollection'\n",
    "df = pd.read_table('SMSSpamCollection.unknown', names=('label', 'sms_message'))\n",
    "\n",
    "# Output printing out first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms_message</th>\n",
       "      <th>length of message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                        sms_message  length of message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...                111\n",
       "1   ham                      Ok lar... Joking wif u oni...                 29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...                155\n",
       "3   ham  U dun say so early hor... U c already then say...                 49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...                 61"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"length of message\"] = df[\"sms_message\"].apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x1ff1a1cf640>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "# To visualize the length of the messages based on the label\n",
    "asdf = sns.FacetGrid(data = df, col = 'label')\n",
    "asdf.map(sns.distplot, 'length of message', kde = False, hist_kws = dict(edgecolor = \"k\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: Data Preprocessing ###\n",
    "\n",
    "Now that we have a basic understanding of what our dataset looks like, let's convert our labels to binary variables, 0 to represent 'ham'(i.e. not spam) and 1 to represent 'spam' for ease of computation. \n",
    "\n",
    "You might be wondering why do we need to do this step? The answer to this lies in how scikit-learn handles inputs. Scikit-learn only deals with numerical values and hence if we were to leave our label values as strings, scikit-learn would do the conversion internally(more specifically, the string labels will be cast to unknown float values). \n",
    "\n",
    "Our model would still be able to make predictions if we left our labels as strings but we could have issues later when calculating performance metrics, for example when calculating our precision and recall scores. Hence, to avoid unexpected 'gotchas' later, it is good practice to have our categorical values be fed into our model as integers. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    ">**Instructions:**\n",
    "* Convert the values in the 'label' column to numerical values using map method as follows:\n",
    "{'ham':0, 'spam':1} This maps the 'ham' value to 0 and the 'spam' value to 1.\n",
    "* Also, to get an idea of the size of the dataset we are dealing with, print out number of rows and columns using \n",
    "'shape'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TO DO\n",
    "# Solution\n",
    "df[\"label\"] = df[\"label\"].map({\"spam\": 1, \"ham\": 0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                        sms_message\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 5572\n",
      "Number of columns: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows:\", df.shape[0])\n",
    "print(\"Number of columns:\", df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                        sms_message\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Bag of Words ###\n",
    "\n",
    "What we have here in our data set is a large collection of text data (5,572 rows of data). Most ML algorithms rely on numerical data to be fed into them as input, and email/sms messages are usually text heavy. \n",
    "\n",
    "Here we'd like to introduce the Bag of Words (BoW) concept which is a term used to specify the problems that have a 'bag of words' or a collection of text data that needs to be worked with. The basic idea of BoW is to take a piece of text and count the frequency of the words in that text. It is important to note that the BoW concept treats each word individually and the order in which the words occur does not matter. \n",
    "\n",
    "Using a process which we will go through now, we can convert a collection of documents to a matrix, with each document being a row and each word (token) being the column, and the corresponding (row, column) values being the frequency of occurrence of each word or token in that document.\n",
    "\n",
    "For example: \n",
    "\n",
    "Let's say we have 4 documents, which are text messages\n",
    "in our case, as follows:\n",
    "\n",
    "`['Hello, how are you!',\n",
    "'Win money, win from home.',\n",
    "'Call me now',\n",
    "'Hello, Call you tomorrow?']`\n",
    "\n",
    "Our objective here is to convert this set of texts to a frequency distribution matrix, as follows:\n",
    "\n",
    "<img src=\"2.jpg\">\n",
    "\n",
    "\n",
    "Here as we can see, the documents are numbered in the rows, and each word is a column name, with the corresponding value being the frequency of that word in the document.\n",
    "\n",
    "Let's break this down and see how we can do this conversion using a small set of documents.\n",
    "\n",
    "To handle this, we will be using sklearn's \n",
    "[count vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) method which does the following:\n",
    "\n",
    "* It tokenizes the string (separates the string into individual words) and gives an integer ID to each token.\n",
    "* It counts the occurrence of each of those tokens.\n",
    "\n",
    "**Please Note:** \n",
    "\n",
    "* The CountVectorizer method automatically converts all tokenized words to their lower case form so that it does not treat words like 'He' and 'he' differently. It does this using the `lowercase` parameter which is by default set to `True`.\n",
    "\n",
    "* It also ignores all punctuation so that words followed by a punctuation mark (for example: 'hello!') are not treated differently than the same words not prefixed or suffixed by a punctuation mark (for example: 'hello'). It does this using the `token_pattern` parameter which has a default regular expression which selects tokens of 2 or more alphanumeric characters.\n",
    "\n",
    "* The third parameter to take note of is the `stop_words` parameter. Stop words refer to the most commonly used words in a language. They include words like 'am', 'an', 'and', 'the', etc. By setting this parameter value to `english`, CountVectorizer will automatically ignore all words (from our input text) that are found in the built in list of English stop words in scikit-learn. This is extremely helpful as stop words can skew our calculations when we are trying to find certain key words that are indicative of spam.\n",
    "\n",
    "We will dive into the application of each of these into our model in a later step, but for now it is important to be aware of such preprocessing techniques available to us when dealing with textual data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Implementing Bag of Words from scratch ###\n",
    "\n",
    "Before we dive into scikit-learn's Bag of Words (BoW) library to do the dirty work for us, let's implement it ourselves first so that we can understand what's happening behind the scenes. \n",
    "\n",
    "**Step 1: Convert all strings to their lower case form.**\n",
    "\n",
    "Let's say we have a document set:\n",
    "\n",
    "```\n",
    "documents = ['Hello, how are you!',\n",
    "             'Win money, win from home.',\n",
    "             'Call me now.',\n",
    "             'Hello, Call hello you tomorrow?']\n",
    "```\n",
    ">>**Instructions:**\n",
    "* Convert all the strings in the documents set to their lower case. Save them into a list called 'lower_case_documents'. You can convert strings to their lower case in python by using the lower() method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO\n",
    "#Solution:\n",
    "documents = ['Hello, how are you!',\n",
    "             'Win money, win from home.',\n",
    "             'Call me now.',\n",
    "             'Hello, Call hello you tomorrow?']\n",
    "\n",
    "# Convert all strings to lowercase, this was called 'lowercase documents'\n",
    "lower_case_documents = [doc.lower() for doc in documents]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello, how are you!', 'win money, win from home.', 'call me now.', 'hello, call hello you tomorrow?']\n"
     ]
    }
   ],
   "source": [
    "print(lower_case_documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Removing all punctuation**\n",
    "\n",
    ">>**Instructions:**\n",
    "Remove all punctuation from the strings in the document set. Save the strings into a list called \n",
    "'sans_punctuation_documents'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are you\n",
      "win money win from home\n",
      "call me now\n",
      "hello call hello you tomorrow\n"
     ]
    }
   ],
   "source": [
    "# TO DO\n",
    "# Solution\n",
    "import string\n",
    "\n",
    "# Document set with strings containing punctuation\n",
    "document_set = lower_case_documents\n",
    "# Remove punctuation from each string in the document set\n",
    "sans_punctuation_documents = []\n",
    "for document in document_set:\n",
    "    document_no_punctuation = \"\".join([char for char in document if char not in string.punctuation])\n",
    "    sans_punctuation_documents.append(document_no_punctuation)\n",
    "\n",
    "# Print the document set without punctuation\n",
    "for document in sans_punctuation_documents:\n",
    "    print(document)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#print(sans_punctuation_documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Tokenization**\n",
    "\n",
    "Tokenizing a sentence in a document set means splitting up the sentence into individual words using a delimiter. The delimiter specifies what character we will use to identify the beginning and  end of a word. Most commonly, we use a single space as the delimiter character for identifying words, and this is true in our documents in this case also."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    ">>**Instructions:**\n",
    "Tokenize the strings stored in 'sans_punctuation_documents' using the split() method. Store the final document set \n",
    "in a list called 'preprocessed_documents'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'how', 'are', 'you']\n",
      "['win', 'money', 'win', 'from', 'home']\n",
      "['call', 'me', 'now']\n",
      "['hello', 'call', 'hello', 'you', 'tomorrow']\n"
     ]
    }
   ],
   "source": [
    "# TO DO\n",
    "# Solution\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the strings using split() method\n",
    "preprocessed_documents = []\n",
    "for document in sans_punctuation_documents:\n",
    "    tokens = document.split()\n",
    "    preprocessed_documents.append(tokens)\n",
    "\n",
    "# Print the tokenized documents\n",
    "for tokens in preprocessed_documents:\n",
    "    print(tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(preprocessed_documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Count frequencies**\n",
    "\n",
    "Now that we have our document set in the required format, we can proceed to counting the occurrence of each word in each document of the document set. We will use the `Counter` method from the Python `collections` library for this purpose. \n",
    "\n",
    "`Counter` counts the occurrence of each item in the list and returns a dictionary with the key as the item being counted and the corresponding value being the count of that item in the list. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    ">>**Instructions:**\n",
    "Using the Counter() method and preprocessed_documents as the input, create a dictionary with the keys being each word in each document and the corresponding values being the frequency of occurrence of that word. Save each Counter dictionary as an item in a list called 'frequency_list'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 2, 'how': 1, 'are': 1, 'you': 1, 'win': 2, 'money': 1, 'from': 1, 'home': 1, 'call': 1, 'me': 1, 'now': 1, 'tomorrow': 1}\n"
     ]
    }
   ],
   "source": [
    "# TO DO\n",
    "# Solution\n",
    "\n",
    "\n",
    "# Create a dictionary with word frequencies\n",
    "frequency_list = {}\n",
    "for document in preprocessed_documents:\n",
    "    frequency_list.update(Counter(document))\n",
    "\n",
    "# Print the word frequencies\n",
    "print(frequency_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "   \n",
    "# pprint.pprint(frequency_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have implemented the Bag of Words process from scratch! As we can see in our previous output, we have a frequency distribution dictionary which gives a clear view of the text that we are dealing with.\n",
    "\n",
    "We should now have a solid understanding of what is happening behind the scenes in the `sklearn.feature_extraction.text.CountVectorizer` method of scikit-learn. \n",
    "\n",
    "We will now implement `sklearn.feature_extraction.text.CountVectorizer` method in the next step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3: Implementing Bag of Words in scikit-learn ###\n",
    "\n",
    "Now that we have implemented the BoW concept from scratch, let's go ahead and use scikit-learn to do this process in a clean and succinct way. We will use the same document set as we used in the previous step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Here we will look to create a frequency matrix on a smaller document set to make sure we understand how the \n",
    "document-term matrix generation happens. We have created a sample document set 'documents'.\n",
    "'''\n",
    "documents = ['Hello, how are you!',\n",
    "                'Win money, win from home.',\n",
    "                'Call me now.',\n",
    "                'Hello, Call hello you tomorrow?']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>**Instructions:**\n",
    "Import the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called 'count_vector'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Solution\n",
    "'''\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vector = CountVectorizer()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data preprocessing with CountVectorizer()**\n",
    "\n",
    "In Step 2.2, we implemented a version of the CountVectorizer() method from scratch that entailed cleaning our data first. This cleaning involved converting all of our data to lower case and removing all punctuation marks. CountVectorizer() has certain parameters which take care of these steps for us. They are:\n",
    "\n",
    "* `lowercase = True`\n",
    "    \n",
    "    The `lowercase` parameter has a default value of `True` which converts all of our text to its lower case form.\n",
    "\n",
    "\n",
    "* `token_pattern = (?u)\\\\b\\\\w\\\\w+\\\\b`\n",
    "    \n",
    "    The `token_pattern` parameter has a default regular expression value of `(?u)\\\\b\\\\w\\\\w+\\\\b` which ignores all punctuation marks and treats them as delimiters, while accepting alphanumeric strings of length greater than or equal to 2, as individual tokens or words.\n",
    "\n",
    "\n",
    "* `stop_words`\n",
    "\n",
    "    The `stop_words` parameter, if set to `english` will remove all words from our document set that match a list of English stop words defined in scikit-learn. Considering the small size of our dataset and the fact that we are dealing with SMS messages and not larger text sources like e-mail, we will not use stop words, and we won't be setting this parameter value.\n",
    "\n",
    "You can take a look at all the parameter values of your `count_vector` object by simply printing out the object as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer()\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Practice node:\n",
    "Print the 'count_vector' object which is an instance of 'CountVectorizer()'\n",
    "'''\n",
    "# No need to revise this code\n",
    "print(count_vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    ">>**Instructions:**\n",
    "Fit your document dataset to the CountVectorizer object you have created using fit(), and get the list of words \n",
    "which have been categorized as features using the get_feature_names() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['are', 'call', 'from', 'hello', 'home', 'how', 'me', 'money',\n",
       "       'now', 'tomorrow', 'win', 'you'], dtype=object)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Solution:\n",
    "'''\n",
    "# No need to revise this code\n",
    "count_vector.fit(documents)\n",
    "count_vector.get_feature_names_out()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get_feature_names()` method returns our feature names for this dataset, which is the set of words that make up our vocabulary for 'documents'."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>**Instructions:**\n",
    "Create a matrix with each row representing one of the 4 documents, and each column representing a word (feature name). \n",
    "Each value in the matrix will represent the frequency of the word in that column occurring in the particular document in that row. \n",
    "You can do this using the transform() method of CountVectorizer, passing in the document data set as the argument. The transform() method returns a matrix of NumPy integers, which you can convert to an array using\n",
    "toarray(). Call the array 'doc_array'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello, how are you!',\n",
       " 'Win money, win from home.',\n",
       " 'Call me now.',\n",
       " 'Hello, Call hello you tomorrow?']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents # Original doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['are', 'call', 'from', 'hello', 'home', 'how', 'me', 'money',\n",
       "       'now', 'tomorrow', 'win', 'you'], dtype=object)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vector.get_feature_names_out() # features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 2, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Solution\n",
    "'''\n",
    "# No need to revise this code\n",
    "\n",
    "doc_array = count_vector.transform(documents).toarray()\n",
    "doc_array # number of times for each word in the line"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a clean representation of the documents in terms of the frequency distribution of the words in them. To make it easier to understand our next step is to convert this array into a dataframe and name the columns appropriately."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>**Instructions:**\n",
    "Convert the 'doc_array' we created into a dataframe, with the column names as the words (feature names). Call the dataframe 'frequency_matrix'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>call</th>\n",
       "      <th>from</th>\n",
       "      <th>hello</th>\n",
       "      <th>home</th>\n",
       "      <th>how</th>\n",
       "      <th>me</th>\n",
       "      <th>money</th>\n",
       "      <th>now</th>\n",
       "      <th>tomorrow</th>\n",
       "      <th>win</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   are  call  from  hello  home  how  me  money  now  tomorrow  win  you\n",
       "0    1     0     0      1     0    1   0      0    0         0    0    1\n",
       "1    0     0     1      0     1    0   0      1    0         0    2    0\n",
       "2    0     1     0      0     0    0   1      0    1         0    0    0\n",
       "3    0     1     0      2     0    0   0      0    0         1    0    1"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO\n",
    "# Solution ( create the frequency_matrix)\n",
    "frequency_matrix = pd.DataFrame(doc_array, columns=count_vector.get_feature_names_out())\n",
    "\n",
    "frequency_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have successfully implemented a Bag of Words problem for a document dataset that we created. \n",
    "\n",
    "One potential issue that can arise from using this method is that if our dataset of text is extremely large (say if we have a large collection of news articles or email data), there will be certain values that are more common than others simply due to the structure of the language itself. For example, words like 'is', 'the', 'an', pronouns, grammatical constructs, etc., could skew our matrix and affect our analyis. \n",
    "\n",
    "There are a couple of ways to mitigate this. One way is to use the `stop_words` parameter and set its value to `english`. This will automatically ignore all the words in our input text that are found in a built-in list of English stop words in scikit-learn.\n",
    "\n",
    "Another way of mitigating this is by using the [tfidf](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) method. This method is out of scope for the context of this lesson."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1: Training and testing sets ###\n",
    "\n",
    "Now that we understand how to use the Bag of Words approach, we can return to our original, larger UCI dataset and proceed with our analysis. Our first step is to split our dataset into a training set and a testing set so we can first train, and then test our model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">>**Instructions:**\n",
    "Split the dataset into a training and testing set using the train_test_split method in sklearn, and print out the number of rows we have in each of our training and testing data. Split the data\n",
    "using the following variables:\n",
    "* `X_train` is our training data for the 'sms_message' column.\n",
    "* `y_train` is our training data for the 'label' column\n",
    "* `X_test` is our testing data for the 'sms_message' column.\n",
    "* `y_test` is our testing data for the 'label' column. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the total set: 5572\n",
      "Number of rows in training set: 4179\n",
      "Number of rows in testing set: 1393\n"
     ]
    }
   ],
   "source": [
    "# TO DO\n",
    "# Solution (split into training and testing sets)\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Assuming your dataset is stored in X (features) and y (target variable)\n",
    "X = df['sms_message']\n",
    "y = df['label']\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Print the number of rows in each set\n",
    "print('Number of rows in the total set: {}'.format(df.shape[0]))\n",
    "print(\"Number of rows in training set:\", X_train.shape[0])\n",
    "print(\"Number of rows in testing set:\", X_test.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print('Number of rows in the total set: {}'.format(df.shape[0]))\n",
    "#print('Number of rows in the training set: {}'.format(X_train.shape[0]))\n",
    "#print('Number of rows in the test set: {}'.format(X_test.shape[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2: Applying Bag of Words processing to our dataset. ###\n",
    "\n",
    "Now that we have split the data, our next objective is to follow the steps from \"Step 2: Bag of Words,\" and convert our data into the desired matrix format. To do this we will be using CountVectorizer() as we did before. There are two  steps to consider here:\n",
    "\n",
    "* First, we have to fit our training data (`X_train`) into `CountVectorizer()` and return the matrix.\n",
    "* Secondly, we have to transform our testing data (`X_test`) to return the matrix. \n",
    "\n",
    "Note that `X_train` is our training data for the 'sms_message' column in our dataset and we will be using this to train our model. \n",
    "\n",
    "`X_test` is our testing data for the 'sms_message' column and this is the data we will be using (after transformation to a matrix) to make predictions on. We will then compare those predictions with `y_test` in a later step. \n",
    "\n",
    "For now, we have provided the code that does the matrix transformations for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n[Practice Node]\\n\\nThe code for this segment is in 2 parts. First, we are learning a vocabulary dictionary for the training data \\nand then transforming the data into a document-term matrix; secondly, for the testing data we are only \\ntransforming the data into a document-term matrix.\\n\\nThis is similar to the process we followed in Step 2.3.\\n\\nWe will provide the transformed data to students in the variables 'training_data' and 'testing_data'.\\n\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "[Practice Node]\n",
    "\n",
    "The code for this segment is in 2 parts. First, we are learning a vocabulary dictionary for the training data \n",
    "and then transforming the data into a document-term matrix; secondly, for the testing data we are only \n",
    "transforming the data into a document-term matrix.\n",
    "\n",
    "This is similar to the process we followed in Step 2.3.\n",
    "\n",
    "We will provide the transformed data to students in the variables 'training_data' and 'testing_data'.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Solution\n",
    "'''\n",
    "# No need to revise this code\n",
    "\n",
    "# Instantiate the CountVectorizer method\n",
    "count_vector = CountVectorizer()\n",
    "\n",
    "# Fit the training data and then return the matrix\n",
    "training_data = count_vector.fit_transform(X_train)\n",
    "\n",
    "# Transform testing data and return the matrix. Note we are not fitting the testing data into the CountVectorizer()\n",
    "testing_data = count_vector.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1: Bayes Theorem implementation from scratch ###\n",
    "\n",
    "Now that we have our dataset in the format that we need, we can move onto the next portion of our mission which is the  algorithm we will use to make our predictions to classify a message as spam or not spam. Remember that at the start of the mission we briefly discussed the Bayes theorem but now we shall go into a little more detail. In layman's terms, the Bayes theorem calculates the probability of an event occurring, based on certain other probabilities that are related to the event in question. It is composed of \"prior probabilities\" - or just \"priors.\" These \"priors\" are the probabilities that we are aware of, or that are given to us. And Bayes theorem is also composed of the \"posterior probabilities,\" or just \"posteriors,\" which are the probabilities we are looking to compute using the \"priors\". \n",
    "\n",
    "Let us implement the Bayes Theorem from scratch using a simple example. Let's say we are trying to find the odds of an individual having diabetes, given that he or she was tested for it and got a positive result. \n",
    "In the medical field, such probabilities play a very important role as they often deal with life and death situations. \n",
    "\n",
    "We assume the following:\n",
    "\n",
    "`P(D)` is the probability of a person having Diabetes. Its value is `0.01`, or in other words, 1% of the general population has diabetes (disclaimer: these values are assumptions and are not reflective of any actual medical study).\n",
    "\n",
    "`P(Pos)` is the probability of getting a positive test result.\n",
    "\n",
    "`P(Neg)` is the probability of getting a negative test result.\n",
    "\n",
    "`P(Pos|D)` is the probability of getting a positive result on a test done for detecting diabetes, given that you have diabetes. This has a value `0.9`. In other words the test is correct 90% of the time. This is also called the Sensitivity or True Positive Rate.\n",
    "\n",
    "`P(Neg|~D)` is the probability of getting a negative result on a test done for detecting diabetes, given that you do not have diabetes. This also has a value of `0.9` and is therefore correct, 90% of the time. This is also called the Specificity or True Negative Rate.\n",
    "\n",
    "The Bayes formula is as follows:\n",
    "\n",
    "<img src=\"1.jpg\" >\n",
    "\n",
    "* `P(A)` is the prior probability of A occurring independently. In our example this is `P(D)`. This value is given to us.\n",
    "\n",
    "* `P(B)` is the prior probability of B occurring independently. In our example this is `P(Pos)`.\n",
    "\n",
    "* `P(A|B)` is the posterior probability that A occurs given B. In our example this is `P(D|Pos)`. That is, **the probability of an individual having diabetes, given that this individual got a positive test result. This is the value that we are looking to calculate.**\n",
    "\n",
    "* `P(B|A)` is the prior probability of B occurring, given A. In our example this is `P(Pos|D)`. This value is given to us."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting our values into the formula for Bayes theorem we get:\n",
    "\n",
    "`P(D|Pos) = P(D) * P(Pos|D) / P(Pos)`\n",
    "\n",
    "The probability of getting a positive test result `P(Pos)` can be calculated using the Sensitivity and Specificity as follows:\n",
    "\n",
    "`P(Pos) = [P(D) * Sensitivity] + [P(~D) * (1-Specificity))]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nInstructions:\\nCalculate probability of getting a positive test result, P(Pos)\\n'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Instructions:\n",
    "Calculate probability of getting a positive test result, P(Pos)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of getting a positive test result P(Pos) is: {} 0.10800000000000001\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Solution (skeleton code will be provided)\n",
    "'''\n",
    "\n",
    "# No need to revise this code\n",
    "\n",
    "# P(D)\n",
    "p_diabetes = 0.01\n",
    "\n",
    "# P(~D)\n",
    "p_no_diabetes = 0.99\n",
    "\n",
    "# Sensitivity or P(Pos|D)\n",
    "p_pos_diabetes = 0.9\n",
    "\n",
    "# Specificity or P(Neg|~D)\n",
    "p_neg_no_diabetes = 0.9\n",
    "\n",
    "# P(Pos)\n",
    "p_pos = p_diabetes*0.9 + p_no_diabetes* 0.1\n",
    "print('The probability of getting a positive test result P(Pos) is: {}',format(p_pos))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using all of this information we can calculate our posteriors as follows:**\n",
    "    \n",
    "The probability of an individual having diabetes, given that, that individual got a positive test result:\n",
    "\n",
    "`P(D|Pos) = (P(D) * Sensitivity)) / P(Pos)`\n",
    "\n",
    "The probability of an individual not having diabetes, given that, that individual got a positive test result:\n",
    "\n",
    "`P(~D|Pos) = (P(~D) * (1-Specificity)) / P(Pos)`\n",
    "\n",
    "The sum of our posteriors will always equal `1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nInstructions:\\nCompute the probability of an individual having diabetes, given that, that individual got a positive test result.\\nIn other words, compute P(D|Pos).\\n\\nThe formula is: P(D|Pos) = (P(D) * P(Pos|D) / P(Pos)\\n'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Instructions:\n",
    "Compute the probability of an individual having diabetes, given that, that individual got a positive test result.\n",
    "In other words, compute P(D|Pos).\n",
    "\n",
    "The formula is: P(D|Pos) = (P(D) * P(Pos|D) / P(Pos)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of an individual having diabetes, given that that individual got a positive test result is: 0.08333333333333333\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Solution\n",
    "'''\n",
    "\n",
    "# No need to revise this code\n",
    "\n",
    "# P(D|Pos)\n",
    "p_diabetes_pos = p_diabetes * p_pos_diabetes / p_pos\n",
    "print('Probability of an individual having diabetes, given that that individual got a positive test result is:\\\n",
    "',format(p_diabetes_pos)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nInstructions:\\nCompute the probability of an individual not having diabetes, given that, that individual got a positive test result.\\nIn other words, compute P(~D|Pos).\\n\\nThe formula is: P(~D|Pos) = P(~D) * P(Pos|~D) / P(Pos)\\n\\nNote that P(Pos|~D) can be computed as 1 - P(Neg|~D). \\n\\nTherefore:\\nP(Pos|~D) = p_pos_no_diabetes = 1 - 0.9 = 0.1\\n'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Instructions:\n",
    "Compute the probability of an individual not having diabetes, given that, that individual got a positive test result.\n",
    "In other words, compute P(~D|Pos).\n",
    "\n",
    "The formula is: P(~D|Pos) = P(~D) * P(Pos|~D) / P(Pos)\n",
    "\n",
    "Note that P(Pos|~D) can be computed as 1 - P(Neg|~D). \n",
    "\n",
    "Therefore:\n",
    "P(Pos|~D) = p_pos_no_diabetes = 1 - 0.9 = 0.1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of an individual not having diabetes, given that that individual got a positive test result is: 0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Solution\n",
    "'''\n",
    "\n",
    "# No need to revise this code\n",
    "\n",
    "# P(Pos|~D)\n",
    "p_pos_no_diabetes = 0.1\n",
    "\n",
    "# P(~D|Pos)\n",
    "p_no_diabetes_pos = p_no_diabetes * p_pos_no_diabetes / p_pos\n",
    "print ('Probability of an individual not having diabetes, given that that individual got a positive test result is:\\\n",
    "',p_no_diabetes_pos)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have implemented Bayes Theorem from scratch. Your analysis shows that even if you get a positive test result, there is only an 8.3% chance that you actually have diabetes and a 91.67% chance that you do not have diabetes. This is of course assuming that only 1% of the entire population has diabetes which is only an assumption."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What does the term 'Naive' in 'Naive Bayes' mean ?** \n",
    "\n",
    "The term 'Naive' in Naive Bayes comes from the fact that the algorithm considers the features that it is using to make the predictions to be independent of each other, which may not always be the case. So in our Diabetes example, we are considering only one feature, that is the test result. Say we added another feature, 'exercise'. Let's say this feature has a binary value of `0` and `1`, where the former signifies that the individual exercises less than or equal to 2 days a week and the latter signifies that the individual exercises greater than or equal to 3 days a week. If we had to use both of these features, namely the test result and the value of the 'exercise' feature, to compute our final probabilities, Bayes' theorem would fail. Naive Bayes' is an extension of Bayes' theorem that assumes that all the features are independent of each other. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Naive Bayes implementation using scikit-learn ###\n",
    "\n",
    "Now let's return to our spam classification context. Thankfully, sklearn has several Naive Bayes implementations that we can use, so we do not have to do the math from scratch. We will be using sklearn's `sklearn.naive_bayes` method to make predictions on our SMS messages dataset. \n",
    "\n",
    "Specifically, we will be using the multinomial Naive Bayes algorithm. This particular classifier is suitable for classification with discrete features (such as in our case, word counts for text classification). It takes in integer word counts as its input. On the other hand, Gaussian Naive Bayes is better suited for continuous data as it assumes that the input data has a Gaussian (normal) distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nInstructions:\\n\\nWe have loaded the training data into the variable 'training_data' and the testing data into the \\nvariable 'testing_data'.\\n\\nImport the MultinomialNB classifier and fit the training data into the classifier using fit(). Name your classifier\\n'naive_bayes'. You will be training the classifier using 'training_data' and 'y_train' from our split earlier. \\n\""
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Instructions:\n",
    "\n",
    "We have loaded the training data into the variable 'training_data' and the testing data into the \n",
    "variable 'testing_data'.\n",
    "\n",
    "Import the MultinomialNB classifier and fit the training data into the classifier using fit(). Name your classifier\n",
    "'naive_bayes'. You will be training the classifier using 'training_data' and 'y_train' from our split earlier. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO DO\n",
    "# Solution (train your model)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "naive_bayes = MultinomialNB()\n",
    "\n",
    "naive_bayes.fit(training_data, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nInstructions:\\nNow that our algorithm has been trained using the training data set we can now make some predictions on the test data\\nstored in 'testing_data' using predict(). Save your predictions into the 'predictions' variable.\\n\""
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Instructions:\n",
    "Now that our algorithm has been trained using the training data set we can now make some predictions on the test data\n",
    "stored in 'testing_data' using predict(). Save your predictions into the 'predictions' variable.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO\n",
    "# Solution (make your prediction)\n",
    "predictions = naive_bayes.predict(testing_data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that predictions have been made on our test set, we need to check the accuracy of our predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Evaluating our model ###\n",
    "\n",
    "Now that we have made predictions on our test set, our next goal is to evaluate how well our model is doing. There are various mechanisms for doing so, so first let's review them.\n",
    "\n",
    "**Accuracy** measures how often the classifier makes the correct prediction. Its the ratio of the number of correct predictions to the total number of predictions (the number of test data points).\n",
    "\n",
    "**Precision** tells us what proportion of messages we classified as spam, actually were spam.\n",
    "It is a ratio of true positives (words classified as spam, and which actually are spam) to all positives (all words classified as spam, regardless of whether that was the correct classification). In other words, precision is the ratio of\n",
    "\n",
    "`[True Positives/(True Positives + False Positives)]`\n",
    "\n",
    "**Recall (sensitivity)** tells us what proportion of messages that actually were spam were classified by us as spam.\n",
    "It is a ratio of true positives (words classified as spam, and which actually are spam) to all the words that were actually spam. In other words, recall is the ratio of\n",
    "\n",
    "`[True Positives/(True Positives + False Negatives)]`\n",
    "\n",
    "For classification problems that are skewed in their classification distributions like in our case - for example if we had 100 text messages and only 2 were spam and the other 98 weren't - accuracy by itself is not a very good metric. We could classify 90 messages as not spam (including the 2 that were spam but we classify them as not spam, hence they would be false negatives) and 10 as spam (all 10 false positives) and still get a reasonably good accuracy score. For such cases, precision and recall come in very handy. These two metrics can be combined to get the **F1 score**, which is the weighted average of the precision and recall scores. This score can range from 0 to 1, with 1 being the best possible F1 score."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using all 4 of these metrics to make sure our model does well. For all 4 metrics whose values can range from 0 to 1, having a score as close to 1 as possible is a good indicator of how well our model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nInstructions:\\nCompute the accuracy, precision, recall and F1 scores of your model using your test data 'y_test' and the predictions\\nyou made earlier stored in the 'predictions' variable.\\n\""
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Instructions:\n",
    "Compute the accuracy, precision, recall and F1 scores of your model using your test data 'y_test' and the predictions\n",
    "you made earlier stored in the 'predictions' variable.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9885139985642498\n",
      "Precision: 0.9775280898876404\n",
      "Recall: 0.9354838709677419\n",
      "F1-Score: 0.956043956043956\n"
     ]
    }
   ],
   "source": [
    "# TO DO\n",
    "# Solution (evaluate your performance)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "# Compute precision\n",
    "precision = precision_score(y_test, predictions)\n",
    "\n",
    "# Compute recall\n",
    "recall = recall_score(y_test, predictions)\n",
    "\n",
    "# Compute F1-score\n",
    "f1 = f1_score(y_test, predictions)\n",
    "\n",
    "# Print the performance metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 7: Conclusion ###\n",
    "\n",
    "One of the major advantages that Naive Bayes has over other classification algorithms is its ability to handle an extremely large number of features. In our case, each word is treated as a feature and there are thousands of different words. Also, it performs well even with the presence of irrelevant features and is relatively unaffected by them. The other major advantage it has is its relative simplicity. Naive Bayes' works well right out of the box and tuning its parameters is rarely ever necessary, except usually in cases where the distribution of the data is known. \n",
    "It rarely ever overfits the data. Another important advantage is that its model training and prediction times are very fast for the amount of data it can handle. All in all, Naive Bayes' really is a gem of an algorithm!\n",
    "\n",
    "Congratulations! You have successfully designed a model that can efficiently predict if an SMS message is spam or not!\n",
    "\n",
    "Thank you for learning with us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Index dimension must be 1 or 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[124], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sns\u001b[39m.\u001b[39mcountplot(testing_data[\u001b[39m\"\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\karakurt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\sparse\\_index.py:47\u001b[0m, in \u001b[0;36mIndexMixin.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[1;32m---> 47\u001b[0m     row, col \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_indices(key)\n\u001b[0;32m     49\u001b[0m     \u001b[39m# Dispatch to specialized methods.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(row, INT_TYPES):\n",
      "File \u001b[1;32mc:\\Users\\karakurt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\sparse\\_index.py:159\u001b[0m, in \u001b[0;36mIndexMixin._validate_indices\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    157\u001b[0m         row \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m M\n\u001b[0;32m    158\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(row, \u001b[39mslice\u001b[39m):\n\u001b[1;32m--> 159\u001b[0m     row \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_asindices(row, M)\n\u001b[0;32m    161\u001b[0m \u001b[39mif\u001b[39;00m isintlike(col):\n\u001b[0;32m    162\u001b[0m     col \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(col)\n",
      "File \u001b[1;32mc:\\Users\\karakurt\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\sparse\\_index.py:183\u001b[0m, in \u001b[0;36mIndexMixin._asindices\u001b[1;34m(self, idx, length)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39minvalid index\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m):\n\u001b[1;32m--> 183\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mIndex dimension must be 1 or 2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    185\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39msize \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[1;31mIndexError\u001b[0m: Index dimension must be 1 or 2"
     ]
    }
   ],
   "source": [
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "scores = [accuracy, precision, recall, f1]\n",
    "\n",
    "\n",
    "plt.bar(metrics, scores, color=colors)\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Performance Metrics')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
